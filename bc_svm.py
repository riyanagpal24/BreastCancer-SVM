# -*- coding: utf-8 -*-
"""BC - SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/riyanagpal24/00b18ad637061ada348794358d28f5d9/bc-svm.ipynb
"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0) #random =0 for resuability

svm = SVC()
'''
To optimize the (achieve higher) accuracy of the parameters, we regularize them.

For regularisation following two parameters ar kept in check:
'C' : it classifies the degree of classification for each data set.
Gamma : controls the width of our guassian kernel
'''
svm.fit(x_train, y_train)
print("Accuracy on the training subset {:.3f}".format(svm.score(x_train, y_train))) # 100 : signifies we are over fitting the data
print("Accuracy on the test subset {:.3f}".format(svm.score(x_test, y_test))) # 62.9

import matplotlib.pyplot as plt
# %matplotlib inline 

plt.plot(x_train.min(axis=0), 'o', label='Min')
plt.plot(x_train.max(axis=0), 'v', label='Max')
plt.xlabel('Feature Index')
plt.ylabel('Feature Magnitude in Log Scale')
plt.yscale('log')
plt.legend(loc='upper right')

'''
large diff between the max and min value of each feature. A diff in order of mangnitude.
'''

#to bring the features to the same scale(eg. 0-1 range)
#StandardScaler() or MinMaxScaler functions can be used.

min_train = x_train.min(axis =0) # axis = 0 - y axis

#min of each feature : subtract min from max
range_train = (x_train - min_train).max(axis = 0) # for y-aixs (represented in log values)

#to bring the data into the scale of 0 - 1
x_train_scaled = (x_train - min_train)/range_train

print('Minimum per feature\n{}'.format(x_train_scaled.min(axis=0)))
print('Maximum per feature\n{}'.format(x_train_scaled.max(axis=0)))

#classifier is used with default parameters
x_test_scaled = (x_test - min_train)/range_train
svm = SVC()
svm.fit(x_train_scaled, y_train) #y_train are the labels so no need to scale

print("Accuracy on the training subset {:.3f}".format(svm.score(x_train_scaled, y_train))) #94.8 : underfitting of data
print("Accuracy on the test subset {:.3f}".format(svm.score(x_test_scaled, y_test))) # 95.1

#try to improve accuracy by adjusting hyper-parameter

svm = SVC(C=1000) #C=1000 leads to a more complex and classified model
svm.fit(x_train_scaled, y_train)

print('The accuracy on the training subset: {:.3f}'.format(svm.score(x_train_scaled, y_train))) # 98.8 (no under or over fitting)
print('The accuracy on the test subset: {:.3f}'.format(svm.score(x_test_scaled, y_test))) #97.2

